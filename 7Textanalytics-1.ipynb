{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f830d0b-aba5-4629-ae5a-0179e76066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Analytics\n",
    "# 1. Extract Sample document and apply following document pre-processing methods:\n",
    "#  Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "# 2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "# Frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69bb4fc2-fecd-4749-a049-32badb2ed829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string #Imports Python's string module, which contains a collection of string constants\n",
    "import nltk #Imports Natural Lang Toolkit(nltk) library, which provides tools for processing & analyzing human language data (text).\n",
    "from nltk.tokenize import word_tokenize #Imports word_tokenize function from nltk.tokenize.It splits string (sentence) into individual words or tokens.\n",
    "from nltk.corpus import stopwords #provides list of common words(\"the\", \"and\", \"is\")that are typically removed in text analysis, as they don’t carry much meaning for most NLP tasks.\n",
    "from nltk import pos_tag # This function tags words in a sentence with their part of speech (e.g., noun, verb, adjective).\n",
    "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
    "#PorterStemmer: A stemmer that reduces words to their root form (e.g., \"running\" → \"run\").\n",
    "#WordNetLemmatizer: A lemmatizer that reduces words to their dictionary form (e.g., \"better\" → \"good\"). It uses WordNet, a lexical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc01ddf0-0685-4f3b-a232-0538ff9b3018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Manish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Manish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Manish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Manish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') #These models are used for tokenizing text, i.e., splitting text into words or sentences.\n",
    "nltk.download('averaged_perceptron_tagger_eng') #It tags words in a sentence with their grammatical roles \n",
    "nltk.download('stopwords') #words that are typically removed during text processing because they don't carry much meaning.\n",
    "nltk.download('wordnet') #which is lexical database of English language. It helps to find synonyms, antonyms,& definitions of words.\n",
    "nltk.download('omw-1.4') #Open Multilingual Wordnet (version 1.4). It is an extension of WordNet, supporting multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb60e84-1082-4308-b38e-67ec45771206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'example', 'text', ',', 'to', 'test', 'the', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#Imports word_tokenize function from nltk.tokenize module, which is used to split text into list of individual words or tokens.\n",
    "text=\"this is example text,to test the word filtration.\" #Defines a sample string text that you want to tokenize.\n",
    "tokens=word_tokenize(text) #Applies word_tokenize() function to text string, which splits text into a list of words or tokens.\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e7bd12-8be7-406f-83a1-c5d59b8bbd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'DT'), ('is', 'VBZ'), ('example', 'NN'), ('text', 'NN'), (',', ','), ('to', 'TO'), ('test', 'VB'), ('the', 'DT'), ('word', 'NN'), ('filtration', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(tokens)\n",
    "#Applies the pos_tag() function to the tokens (list of words or tokens), which assigns part-of-speech tag to each token.Each word is tagged with POS label,such as noun(NN),verb (VB),etc.\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83ab6b8-3054-4961-88fa-6b27ae5c0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stop Words Removal:\n",
      " ['example', 'text', 'test', 'word', 'filtration']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#Imports the list of common stopwords(e.g.,\"the\",\"is\",\"and\",etc.) in English from nltk.corpus.stopwords module.\n",
    "#Converts the list of stopwords into a set for faster look-up during filtering.\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
    "#word.lower() not in stop_words: Ensures that the word (in lowercase) is not a stopword.\n",
    "#word not in string.punctuation: Ensures word is not a punctuation mark (using string.punctuation constant).\n",
    "print(\"Tokens after Stop Words Removal:\\n\", filtered_tokens)\n",
    "#Prints the list of tokens after removing the stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac10fa2a-5b6b-4396-b452-8ff8e08d9492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens:\n",
      " ['exampl', 'text', 'test', 'word', 'filtrat']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "#Initializes PorterStemmer from nltk.stem module,which is used to reduce words to their root form (i.e.,stemming).For example,\"running\" becomes \"run\" and \"better\" becomes \"better\".\n",
    "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "#Iterates over each word in filtered_tokens list (which contains tokens after removing stopwords and punctuation).\n",
    "print(\"Stemmed Tokens:\\n\", stemmed)\n",
    "#Prints the list of stemmed tokens, showing the root form of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada355fc-5a19-4910-9a88-3431f10c8791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:\n",
      " ['example', 'text', 'test', 'word', 'filtration']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#Creates instance of WordNetLemmatizer, which is tool from nltk library used to reduce words to their base or root form (known as lemmatization).\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\\n\", lemmatized) #Prints the list of tokens after lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a2eb02-6a93-4119-80ce-71efc3e62a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and   animals       are    clever       dog      fast       fox  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.289695  0.000000  0.380914   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.313316  0.000000  0.000000   \n",
      "2  0.408248  0.408248  0.408248  0.408248  0.000000  0.408248  0.000000   \n",
      "\n",
      "      foxes    garden        in     jumps      lazy      over    sleeps  \\\n",
      "0  0.000000  0.000000  0.000000  0.380914  0.380914  0.380914  0.000000   \n",
      "1  0.000000  0.411973  0.411973  0.000000  0.000000  0.000000  0.411973   \n",
      "2  0.408248  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        the  \n",
      "0  0.579391  \n",
      "1  0.626632  \n",
      "2  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#used to convert collection of text documents into matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features.\n",
    "import pandas as pd\n",
    "documents = [\n",
    "    \"The fox jumps over the lazy dog\",\n",
    "    \"The dog sleeps in the garden\",\n",
    "    \"Foxes are clever and fast animals\"\n",
    "]\n",
    "#Defines a list of 3 text documents (documents) that will be processed to extract TF-IDF features.\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "#Initializes TfidfVectorizer object, which will be used to convert text into matrix of TF-IDF features.\n",
    "tfidf_matrix = tfidf.fit_transform(documents)\n",
    "\n",
    "\n",
    "#Display terms and tf-idf scores\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738fdf2-874e-4ed0-850c-238d97ba2776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
